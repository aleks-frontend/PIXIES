Admin Notify: Email Address = x
Admin Notify: SMTP Server = x
Admin Notify: Sendmail Program = x
Allow Index Entire Site = Allows entire web sites to be indexed into Open Realms in addition to Website Realms.
AllowAnonAdd = x
AllowAnonAdd: Notify Admin = x
AllowAnonAdd: Require User Email= x
AllowBinaryFiles = Allow binary files (Word docs, MP3 files, etc.) in Local Realms. The contents of the file are not indexed - only the filename itself.
AllowSymbolicLinks = Follow symbolic links while building file lists for Local Realms.
Crawler: Days Til Refresh = The number of days before the crawler will become interested in re-visiting a website in a Remote Realm. Note that the crawler does not start re-indexing on it's own. Rather, when you use the "Revisit Old Pages" command, the crawler will use this value to decide whether a page is "Old" or not.
Crawler: Follow Offsite Links = Controls whether to extract links to other sites when parsing an HTML page. For example, web page http://foo.com/foo.htm has links to http://foo.com/bar.htm, and to http://x.com. If Follow Offsite Links = 1, then both links are extracted; otherwise, only the first is extracted. Note that this only governs link extraction - you can still enter any starting URL you wish.
Crawler: Follow Query Strings = Controls whether to extract links that have query strings, such as "http://foo.com/?name=value". Note that this only governs link extraction - you can still enter any starting URL you wish.
Crawler: Ignore Links To = A space-separated list of file extensions. Will not extract links to files with these extensions when parsing an HTML page. By ignoring links that probably won't lead to text-based files, the overall efficiency of the crawler is increased. Note that this only governs link extraction - you can still enter any starting URL you wish.
Crawler: Max Pages Per Batch = Maximum number of web pages that the crawler will request during a single admin action. Setting this too high will increase the likelihood of a server timeout during a crawl session; setting it too low will be less efficient for crawling large numbers of pages. 10 - 40 is a good range. See the "Timeout" setting for another way to manage this.
Crawler: Max Redirects = Maximum number of HTTP and HTML redirects to be followed while trying to request a single page.
Crawler: Minimum Whitespace = Minimum ratio of spaces to total file content. Typical text documents have a whitespace ratio of 1:6 (or 0.16). Binary data has the much lower whitespace ratio of 0.004. The whitespace ratio is used as a test to see whether a file has text content (desired) or binary content (bad), in instances where traditional content-type or -T tests fail.
Crawler: Rogue = Controls whether the web crawler will respect the robots exclusion protocol. Setting this to 1 will cause robots.txt and Robots Meta tags to be ignored.
Crawler: User Agent = The user-agent tag of the crawler; used as an identification string during HTTP requests and robots.txt parsing.
Default Match = The default value to use in the search form. "0" means "Match Any Term". "1" means "Match All Terms". "2" means "Match as Phrase".  (Visitors can still manually choose whichever they want right before they search.)
Delete Index File with Realm = When deleting a realm, the associated index files may be deleted also. To delete index files with the realm, set this setting to 1 (checked). Otherwise, set it to 0 (unchecked).
Ext = Space-separated list of file extensions to be indexed during the creation of Local Realms.
Forbid All Cap Descriptions = When a page description is in all caps, the &parse_html function will convert it to lowercase.
Forbid All Cap Titles = When a page title is in all caps, the &parse_html function will convert it to lowercase.
Handling URL Search Terms = Sometimes visitors type in search terms like "http://www.hotmail.com/" or "www.hotmail.com". They expect to just be taken to the appropriate website, rather than have a search performed on this term. FDSE can handle this three ways: 1. treat the URL as a normal search term; 2. treat the URL as a term, but at the top of the results list, say "Click here to visit URL"; 3. redirect the visitor to the URL immediately.
Hits Per Page = Number of search results to show per page.
Ignore Words = Space-separated list of words that will be stripped from HTML before being stored in the index files. Keep this list full of common words to keep the index file size manageable.
Index ALT Text = Controls whether ALT and TITLE attributes - commonly found in IMG tags - will be stored in the index file.
Index Links = Controls whether the search index will include information about all the links in a web page. This feature allows end users to search for "link:xav.com" and find all web pages that link to "xav.com". However, this adds 5-15% to the size of the index file for a rarely-used feature, and so this is disabled by default.
Language = x
Max Characters: Auto Description = Maximum bytes read from a file to generate a description (when no Meta description tag is present).
Max Characters: Description = Maximum bytes read from the Meta description tag.
Max Characters: File = Maximum bytes read from the underlying file, or from a network web page, before passing it to the HTML parsing function. By keeping this at a reasonable value, the crawler won't choke when it encounters huge files (say, 20 MB). This limit is imposed at a very low level, when the file is first read from disk or read over the network.
Max Characters: Keywords = Maximum bytes allowed from the keywords Meta tag.
Max Characters: Text = Maximum bytes of normal text saved to the index file. For those who need a very lean and mean search engine that indexes only titles, descriptions, and keywords, set this value to 0. Note that this is a very high-level limit, imposed after parsing HTML. Compare with the low-level "Max Characters: File" property.
Max Characters: Title = Maximum bytes allowed for the title.
Max Characters: URL = Will not extract links to URL's longer than this when parsing an HTML page. While other "Max Characters" settings will just truncate strings (like a web page title can be stripped to the first 96 bytes), URL that exceed the maximum length will be ignored altogether. Note that this only governs link extraction - you can still enter any starting URL you wish.
Max Index File Size = This is the maximum size, in bytes, allowed for an index file. After it reaches this size, no additional web pages can be indexed. This feature is mainly used in conjunction with the AllowAnonAdd setting, where end users can be adding their URL's to your search engine on their own. Setting the max size to 0 means there is no maximum size.
Minimum Page Size = The minimum allowed size for indexed web pages, in bytes.
Multiplier: Description = For purposes of weighting search results, a word in the description will count this many times more than a word in the file body. Setting this to zero will cause this field to not be weighted any more heavily than the rest of the document, but will speed up the search engine.
Multiplier: Keyword = For purposes of weighting search results, a word in the keywords will count this many times more than a word in the file body. Setting this to zero will cause this field to not be weighted any more heavily than the rest of the document, but will speed up the search engine.
Multiplier: Title = For purposes of weighting search results, a word in the title will count this many times more than a word in the file body. Setting this to zero will cause this field to not be weighted any more heavily than the rest of the document, but will speed up the search engine.
No Frames = Use alternate no-frames navigation for the admin page.
Parse FDSE-Index-As Header = Allow web pages to control the URL used to reference them, using the proprietary "fdse-index-as" META tag. See help file for more information.
Password = x
Redirector = This is an absolute or relative URL that is inserted before the true URL in the search results listing. Typically it is the path to a CGI script that transparently logs where visitors are going before sending them to their destination.
Require Anon Approval = x
SQL: Database = Name of search engine database on SQL server.
SQL: Enable = Controls whether the search engine uses a database or the filesystem for searches and administrative functions.
SQL: Hostname = Name of SQL server.
SQL: Logfile = Use SQL to store visitor search results (1). If set to 0, will continue to use the search.log.txt file.
SQL: Password = Password for connecting to SQL database.
SQL: Table Name: Addresses = The name of the SQL table holding web page information. Users who have only one database, but who want to run multiple parallel system, must use distinct names for each set of tables. All other users should just keep the defaults.
SQL: Table Name: Logs = Name of table to use for storing visitor searches.
SQL: Table Name: Realms = The name of the SQL table holding realms information. Users who have only one database, but who want to run multiple parallel system, must use distinct names for each set of tables. All other users should just keep the defaults.
SQL: Username = Username for connecting to SQL database.
Show Examples: Enable = In the search results list, will show an example of the search term being used in the document. This is very helpful for putting the search result in context. Enabling this option causes searches to take a little longer. Also, this feature is only really useful if you delete or shrink the "Ignore Words" setting, and then rebuild your index files. Otherwise the examples aren't readable.
Show Examples: Number to Display = When the "Show Examples: Enable" setting is on, this controls how many examples are shown to the end user. The search engine is optimized for the default setting, 1. Increasing to higher values causes searches to take longer.
Sorting: Default Sort Method = Set to: 1 to sort by relevance; 2, reverse relevance; 3, last modified time, with newest entries first; 4, reverse last modified time; 5, last indexed time, with newest entries first; 6, reverse last indexed time.
Sorting: Randomize Equally-Relevant Search Results = When enabled, the search results which have an otherwise equal relevance will be randomly ordered.
Time Interval Between Restarts = Number of seconds of idle time before the FDSE admin script restarts itself during long operations, like an index rebuild. A longer idle interval will reduce the load on the server.
Timeout = The number of seconds before a CGI script will time out on this server. Used to throttle the local file indexing and crawler processes, to prevent a server timeout.
TrustSymbolicLinks = Disable the algorithm that guards against infinite loops when following symbolic links.
UI: Date Format = x
UI: Number Format = x
Use Standard IO = When checked, all network socket operations use the standard, buffered input/output routines. When unchecked, network socket operations will bypass standard I/O. This setting exists to allow you to work around a bug on some Windows NT 4.0 systems which require bypassing standard IO.
Wildcard Match = The Perl regular expression to use in place of "*" in user queries. The default, "\S{0,4}", will match zero to 4 non-whitespace characters. To match any character, including whitespace, use ".{0,4}". To match 0 to 100 characters, use ".{0,100}". Limiting the range of matches to a low number like 4 will improve performance.
security: session timeout = x
UI: Search Form Display = x
Logging: Enable = Save information about visitor searches to a log, for analysis later.
Sorting: Time Sensitive = When enabled, documents modified in the last two days will have their base relevance multiplied by 4; documents modified in the last 4 days have their relevance multiplied by 3; and documents modified in the last 8 days have their relevance multiplied by 2. Use this feature when searching time-sensitive material, like news articles.
Allow Filtered Realms = Allows you to created the special "Filtered" type of realm.
Multi-Line Add-URL Form - Admin = Allows multiple URL's in the Add URL form for administrators
Multi-Line Add-URL Form - Visitors = Allows multiple URL's in the Add URL form for visitors
Network Timeout = Number of seconds before timing out a send or receive. This feature is implemented only on platforms which support IO::Select and alarm().
